{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'lowest', 'newest', 'widest']\n"
     ]
    }
   ],
   "source": [
    "text = \"low lowest newest widest\"\n",
    "print(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(text):\n",
    "    \n",
    "    vocab={}\n",
    "    for word in text.split():\n",
    "        if word in vocab:\n",
    "            vocab[' '.join(word)] +=1\n",
    "        else:\n",
    "            vocab[' '.join(word)] =1\n",
    "    return vocab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. character pair 빈도 세기\n",
    "def get_stats(vocab):\n",
    "    pairs = {}\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += freq\n",
    "            else:  \n",
    "                pairs[pair] = freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=get_stats(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 가장 빈도가 높은 pair 선택\n",
    "best_pair = max(pairs, key=pairs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 새로운 토큰 생성\n",
    "def merge_vocab(pair,vocab):\n",
    "    new_vocab= {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement= ''.join(pair)\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram,replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = merge_vocab(vocab, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Best PAIR : l o w NEW VOCAB : {'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
      "[1] Best PAIR : l o w NEW VOCAB : {'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
      "[2] Best PAIR : l o w NEW VOCAB : {'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
      "[3] Best PAIR : l o w NEW VOCAB : {'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
      "[4] Best PAIR : l o w NEW VOCAB : {'l o w': 1, 'l o w e s t': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n"
     ]
    }
   ],
   "source": [
    "text = 'low lowest newest widest'\n",
    "num_merge = 5\n",
    "\n",
    "vocab =get_vocab(text)\n",
    "for i in range(num_merge):\n",
    "    pairs = vocab\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "    print(f\"[{i}] Best PAIR : {best_pair} NEW VOCAB : {vocab}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: sentencepiece in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (0.1.97)\n",
      "Requirement already satisfied: requests in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: sacremoses in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: boto3 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (1.26.79)\n",
      "Requirement already satisfied: tqdm in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.79 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from boto3->transformers) (1.29.79)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from boto3->transformers) (1.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: six in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from sacremoses->transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.79->boto3->transformers) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from click->sacremoses->transformers) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jiwon/anaconda3/envs/cycle/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 2026, 2171, 2003, 10147, 19291, 15333, 5063, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello My name is Jiwon Jeong.'\n",
    "\n",
    "encoded_list = tokenizer.encode(text,) # Token id\n",
    "\n",
    "print(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello my name is jiwon jeong. [SEP]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042301/1042301 [00:00<00:00, 1067869.19B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 611087.49B/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15496, 2011, 1438, 318, 449, 14246, 261, 3852, 506, 13]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_list = tokenizer.encode(text)\n",
    "encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello My name is Jiwon Jeong.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15496 Hello\n",
      "2011  My\n",
      "1438  name\n",
      "318  is\n",
      "449  J\n",
      "14246 iw\n",
      "261 on\n",
      "3852  Je\n",
      "506 ong\n",
      "13 .\n"
     ]
    }
   ],
   "source": [
    "for token_id in encoded_list:\n",
    "    decoded_text = tokenizer.decode([token_id])\n",
    "    print(token_id, decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8addaabea1cb039ebd0cad50c57d5e545a2dd9c4864cd769240013056fee56e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
